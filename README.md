# candle-llm-chat

åŸºäº [Candle](https://github.com/huggingface/candle) æ¡†æ¶çš„ Rust LLM èŠå¤©åº“ï¼Œæ”¯æŒ GGUF é‡åŒ–æ¨¡å‹ã€æµå¼è¾“å‡ºå’Œ GPU åŠ é€Ÿã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **ç®€æ´ API**: å­—ç¬¦ä¸²æ ‡è¯†ç¬¦é€‰æ‹©æ¨¡å‹ `"qwen3"` / `"qwen3.W3_14b"`
- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**: Qwen3/Llama ç³»åˆ—ï¼Œé€šè¿‡ `models.toml` é…ç½®
- ğŸ“¦ **åŒæ ¼å¼æ”¯æŒ**: GGUF é‡åŒ–æ¨¡å‹ + Safetensors å®Œæ•´æ¨¡å‹
- ğŸ“¡ **æµå¼è¾“å‡º**: å®æ—¶æ‰“å­—æœºæ•ˆæœ
- ğŸš€ **GPU åŠ é€Ÿ**: CUDA æ”¯æŒ
- âš¡ **å¼‚æ­¥è®¾è®¡**: åŸºäº Tokio
- ğŸ§  **æ™ºèƒ½ä¸Šä¸‹æ–‡**: è‡ªåŠ¨è§’è‰²åˆ‡æ¢å’Œæ€è€ƒè¿‡ç¨‹è¿‡æ»¤
- ğŸŒ **ç¯å¢ƒå˜é‡é…ç½®**: æ”¯æŒ `HF_ENDPOINT`ã€`HF_TOKEN` ç­‰ç¯å¢ƒå˜é‡

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Rust å·¥å…·é“¾ (æ¨èæœ€æ–°ç¨³å®šç‰ˆ)
- CUDA å·¥å…·åŒ… (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)
- `gguf-utils` (å¯é€‰ï¼Œç”¨äºåˆ†ç‰‡æ¨¡å‹åˆå¹¶): `cargo install gguf-utils`

### å®‰è£…

```bash
git clone https://github.com/your-username/candle-llm-chat.git
cd candle-llm-chat
```

**æ¨èï¼šè®¾ç½®ç¯å¢ƒå˜é‡**

```powershell
# Windows PowerShellï¼ˆå›½å†…ç”¨æˆ·æ¨èè®¾ç½®é•œåƒï¼‰
$env:HF_ENDPOINT = "https://hf-mirror.com"
```

```bash
# Linux/macOSï¼ˆå›½å†…ç”¨æˆ·æ¨èè®¾ç½®é•œåƒï¼‰
export HF_ENDPOINT="https://hf-mirror.com"
```

### ç¯å¢ƒå˜é‡é…ç½®

é¡¹ç›®æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ï¼Œæ— éœ€ç»´æŠ¤é…ç½®æ–‡ä»¶ï¼š

**Windows PowerShell:**

```powershell
# è®¾ç½® HuggingFace é•œåƒç«™ç‚¹ï¼ˆå›½å†…ç”¨æˆ·æ¨èï¼‰
$env:HF_ENDPOINT = "https://hf-mirror.com"


# è®¾ç½® HuggingFace Tokenï¼ˆè®¿é—®ç§æœ‰æ¨¡å‹æˆ–æé«˜é™é¢ï¼‰
$env:HF_TOKEN = "hf_your_token_here"
```

**Linux/macOS:**

```bash
# è®¾ç½® HuggingFace é•œåƒç«™ç‚¹ï¼ˆå›½å†…ç”¨æˆ·æ¨èï¼‰
export HF_ENDPOINT="https://hf-mirror.com"

# è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
export HF_HOME="/data/huggingface_cache"

# è®¾ç½® HuggingFace Tokenï¼ˆè®¿é—®ç§æœ‰æ¨¡å‹æˆ–æé«˜é™é¢ï¼‰
export HF_TOKEN="hf_your_token_here"
```

**éªŒè¯ç¯å¢ƒå˜é‡è®¾ç½®:**

```powershell
# Windows PowerShell
echo $env:HF_ENDPOINT
echo $env:HF_TOKEN

# Linux/macOS
echo $HF_ENDPOINT
echo $HF_TOKEN
```

### åŸºæœ¬ä½¿ç”¨

```rust
use candle_llm_chat::pipe::TextGeneration;
use futures_util::{StreamExt, pin_mut};

// ä½¿ç”¨é»˜è®¤æ¨¡å‹ (Qwen3-8B)
let mut text_gen = TextGeneration::default().await?;

let stream = text_gen.chat("ä½ å¥½");
pin_mut!(stream);

while let Some(Ok(token)) = stream.next().await {
    print!("{}", token);
}
```

### è¿è¡Œæµ‹è¯•

```bash
# äº¤äº’å¼èŠå¤©
cargo test --lib pipe::tests::test_pipeline -- --nocapture

# é¢„è®¾å¯¹è¯
cargo test --lib pipe::tests::test_prompt -- --nocapture
```

### ç½‘ç»œé…ç½®

**ç¯å¢ƒå˜é‡æ–¹å¼ï¼ˆæ¨èï¼‰:**

```powershell
# Windows PowerShell - ä½¿ç”¨å›½å†…é•œåƒ
$env:HF_ENDPOINT = "https://hf-mirror.com"
```

**ä»£ç æ–¹å¼ï¼ˆå¯é€‰ï¼‰:**

```rust
use candle_llm_chat::utils::proxy::ProxyGuard;

let _proxy = ProxyGuard::new(7890); // è‡ªåŠ¨æ¸…ç†çš„ä»£ç†è®¾ç½®
```

## âš™ï¸ é…ç½®ä¸ä½¿ç”¨

### é€‰æ‹©æ¨¡å‹

```rust
// ä½¿ç”¨æ¶æ„é»˜è®¤æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3").await?;

// ä½¿ç”¨ GGUF é‡åŒ–æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3.W3_14b").await?;

// ä½¿ç”¨ Safetensors å®Œæ•´æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3.W3_8b_full").await?;

// ä½¿ç”¨ Llama æ¨¡å‹
let text_gen = TextGeneration::with_default_config("llama.DeepseekR1Llama8b").await?;
```

### è‡ªå®šä¹‰æ¨ç†å‚æ•°

```rust
use candle_llm_chat::model::config::InferenceConfig;

let mut config = InferenceConfig::default();
config.temperature = 0.7;        // æ§åˆ¶éšæœºæ€§
config.sample_len = 2000;        // æœ€å¤§ç”Ÿæˆé•¿åº¦
config.repeat_penalty = 1.1;     // é‡å¤æƒ©ç½š

let mut text_gen = TextGeneration::new("qwen3", config).await?;
```

### é…ç½®æ–‡ä»¶

**`models.toml`** - æ¨¡å‹ä»“åº“é…ç½®ï¼š

```toml
# GGUF é‡åŒ–æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
[qwen3.W3_8b]
model_repo = "Qwen/Qwen3-8B-GGUF"
model_file = "Qwen3-8B-Q4_K_M"
tokenizer_repo = "Qwen/Qwen3-8B"  # å¯é€‰ï¼Œæœªé…ç½®æ—¶è‡ªåŠ¨ä½¿ç”¨ model_repo
default = true

# Safetensors å®Œæ•´æ¨¡å‹ - tokenizer_repo æœªé…ç½®æ—¶è‡ªåŠ¨ä½¿ç”¨ model_repo
[qwen3.W3_8b_full]
model_repo = "Qwen/Qwen3-8B"
model_file = "model.safetensors"
model_type = "safetensors"
```

> **æ³¨æ„**: é¡¹ç›®ç°åœ¨ä½¿ç”¨ç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ï¼Œä¸å†éœ€è¦ `config.toml` æ–‡ä»¶ã€‚HuggingFace Token ç­‰é…ç½®è¯·é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ã€‚

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·äº¤äº’å±‚"
        A[ç”¨æˆ·è¾“å…¥] --> B[TextGeneration::chat]
    end

    subgraph "é…ç½®ç®¡ç†å±‚"
        MR[ModelRegistry<br/>æ¨¡å‹æ³¨å†Œè¡¨] --> HI[HubInfo<br/>æ¨¡å‹ä»“åº“ä¿¡æ¯]
        MT[models.toml] --> MR
        HI --> ML[ModelLoader<br/>æ¨¡å‹åŠ è½½å™¨]
    end

    subgraph "æ ¸å¿ƒç»„ä»¶"
        C[ChatContext<br/>èŠå¤©ä¸Šä¸‹æ–‡ç®¡ç†] --> TG[TextGeneration<br/>æ–‡æœ¬ç”Ÿæˆç®¡é“]
        ML --> TG
        IC[InferenceConfig<br/>æ¨ç†é…ç½®] --> TG
        F[TokenOutputStream<br/>Tokenæµå¤„ç†] --> TG
        G[LogitsProcessor<br/>é‡‡æ ·å¤„ç†] --> TG
    end

    subgraph "æ¨¡å‹æŠ½è±¡å±‚"
        FW[Forward Trait<br/>ç»Ÿä¸€æ¨ç†æ¥å£] --> MW[ModelWeightså®ç°]
        MW --> MW1[quantized_qwen3::ModelWeights]
        MW --> MW2[quantized_llama::ModelWeights]
    end

    subgraph "æ¨¡å‹å®ç°å±‚"
        MW1 --> H1[Qwen3 GGUFæ¨¡å‹æ–‡ä»¶]
        MW2 --> H2[Llama GGUFæ¨¡å‹æ–‡ä»¶]
        I[Tokenizer<br/>åˆ†è¯å™¨] --> TG
    end

    subgraph "åº•å±‚æ¡†æ¶"
        K[Candle Framework<br/>æœºå™¨å­¦ä¹ æ¡†æ¶]
        L[CUDA Support<br/>GPUåŠ é€Ÿ]
        M[HuggingFace Hub<br/>æ¨¡å‹ä»“åº“]
    end

    subgraph "å·¥å…·ç»„ä»¶"
        N[ProxyGuard<br/>ä»£ç†è®¾ç½®] --> M
        O[gguf-utils<br/>æ¨¡å‹åˆ†ç‰‡åˆå¹¶] --> H1
        O --> H2
    end

    B --> C
    TG --> P[Stream Output<br/>æµå¼è¾“å‡º]
    P --> Q[å®æ—¶å“åº”æ˜¾ç¤º]

    H1 --> M
    H2 --> M
    I --> M
    MW1 --> K
    MW2 --> K
    K --> L

    style MR fill:#fff3e0
    style HI fill:#e3f2fd
    style FW fill:#f1f8e9
    style C fill:#e1f5fe
    style TG fill:#f3e5f5
    style P fill:#e8f5e8
```

### æ ¸å¿ƒè®¾è®¡

**é…ç½®é©±åŠ¨**: é€šè¿‡ `models.toml` ç®¡ç†æ¨¡å‹ï¼Œå­—ç¬¦ä¸²æ ‡è¯†ç¬¦é€‰æ‹© (`"qwen3"` æˆ– `"qwen3.W3_14b"`)

**ç»Ÿä¸€æ¥å£**: `Forward` trait æŠ½è±¡æ‰€æœ‰æ¨¡å‹æ¨ç†ï¼Œé€šè¿‡å®è‡ªåŠ¨å®ç°

**å¼‚æ­¥ä¼˜å…ˆ**: æ¨¡å‹åŠ è½½å’Œæ¨ç†å…¨å¼‚æ­¥ï¼ŒåŸºäº Tokio å’Œ async-stream

## æ‰©å±•æ–°æ¨¡å‹

æ·»åŠ æ–°æ¨¡å‹å˜ä½“åªéœ€åœ¨ `models.toml` ä¸­é…ç½®ï¼š

**GGUF é‡åŒ–æ¨¡å‹ï¼š**

```toml
[qwen3.W3_72b]
model_repo = "Qwen/Qwen3-72B-GGUF"
model_file = "Qwen3-72B-Q4_K_M"
tokenizer_repo = "Qwen/Qwen3-72B"
```

**Safetensors å®Œæ•´æ¨¡å‹ï¼š**

```toml
[qwen3.W3_4b_full]
model_repo = "Qwen/Qwen3-4B"
model_file = "model.safetensors"
tokenizer_repo = "Qwen/Qwen3-4B"
model_type = "safetensors"
```

ç„¶åç›´æ¥ä½¿ç”¨ï¼š

```rust
let text_gen = TextGeneration::with_default_config("qwen3.W3_72b").await?;
let text_gen_full = TextGeneration::with_default_config("qwen3.W3_4b_full").await?;
```

æ·»åŠ æ–°æ¶æ„éœ€è¦åœ¨ `ModelLoader` ä¸­å®ç°åŠ è½½é€»è¾‘ã€‚

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚
