# candle-llm-chat

åŸºäº [Candle](https://github.com/huggingface/candle) æ¡†æ¶çš„ Rust LLM èŠå¤©åº“ï¼Œæ”¯æŒ GGUF é‡åŒ–æ¨¡å‹ã€æµå¼è¾“å‡ºå’Œ GPU åŠ é€Ÿã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **ç®€æ´ API**: å­—ç¬¦ä¸²æ ‡è¯†ç¬¦é€‰æ‹©æ¨¡å‹ `"qwen3"` / `"qwen3.8b_q4"`
- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**: Qwen3/Llama ç³»åˆ—ï¼Œé€šè¿‡ `models.toml` é…ç½®
- ğŸ“¦ **åŒæ ¼å¼æ”¯æŒ**: GGUF é‡åŒ–æ¨¡å‹ + Safetensors å®Œæ•´æ¨¡å‹
- ğŸ“¡ **æµå¼è¾“å‡º**: å®æ—¶æ‰“å­—æœºæ•ˆæœ
- ğŸš€ **GPU åŠ é€Ÿ**: CUDA æ”¯æŒ
- âš¡ **å¼‚æ­¥è®¾è®¡**: åŸºäº Tokio
- ğŸ§  **æ™ºèƒ½ä¸Šä¸‹æ–‡**: è‡ªåŠ¨è§’è‰²åˆ‡æ¢å’Œæ€è€ƒè¿‡ç¨‹è¿‡æ»¤
- ğŸŒ **æ™ºèƒ½é…ç½®**: tokenizer_repo è‡ªåŠ¨å¡«å……ï¼Œçº¦å®šä¼˜äºé…ç½®

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Rust å·¥å…·é“¾ (æ¨èæœ€æ–°ç¨³å®šç‰ˆ)
- CUDA å·¥å…·åŒ… (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)
- `gguf-utils` (å¯é€‰ï¼Œç”¨äºåˆ†ç‰‡æ¨¡å‹åˆå¹¶): `cargo install gguf-utils`

### å®‰è£…

```bash
git clone https://github.com/your-username/candle-llm-chat.git
cd candle-llm-chat
```

**æ¨èï¼šè®¾ç½®ç¯å¢ƒå˜é‡**

```powershell
# Windows PowerShellï¼ˆå›½å†…ç”¨æˆ·æ¨èè®¾ç½®é•œåƒï¼‰
$env:HF_ENDPOINT = "https://hf-mirror.com"
```

```bash
# Linux/macOSï¼ˆå›½å†…ç”¨æˆ·æ¨èè®¾ç½®é•œåƒï¼‰
export HF_ENDPOINT="https://hf-mirror.com"
```

### ç¯å¢ƒå˜é‡é…ç½®

é¡¹ç›®æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ï¼Œæ— éœ€ç»´æŠ¤é…ç½®æ–‡ä»¶ï¼š

**Windows PowerShell:**

```powershell
# è®¾ç½® HuggingFace é•œåƒç«™ç‚¹ï¼ˆå›½å†…ç”¨æˆ·æ¨èï¼‰
$env:HF_ENDPOINT = "https://hf-mirror.com"


# è®¾ç½® HuggingFace Tokenï¼ˆè®¿é—®ç§æœ‰æ¨¡å‹æˆ–æé«˜é™é¢ï¼‰
$env:HF_TOKEN = "hf_your_token_here"
```

**Linux/macOS:**

```bash
# è®¾ç½® HuggingFace é•œåƒç«™ç‚¹ï¼ˆå›½å†…ç”¨æˆ·æ¨èï¼‰
export HF_ENDPOINT="https://hf-mirror.com"

# è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰
export HF_HOME="/data/huggingface_cache"

# è®¾ç½® HuggingFace Tokenï¼ˆè®¿é—®ç§æœ‰æ¨¡å‹æˆ–æé«˜é™é¢ï¼‰
export HF_TOKEN="hf_your_token_here"
```

**éªŒè¯ç¯å¢ƒå˜é‡è®¾ç½®:**

```powershell
# Windows PowerShell
echo $env:HF_ENDPOINT
echo $env:HF_TOKEN

# Linux/macOS
echo $HF_ENDPOINT
echo $HF_TOKEN
```

### åŸºæœ¬ä½¿ç”¨

```rust
use candle_llm_chat::pipe::TextGeneration;
use futures_util::{StreamExt, pin_mut};

// ä½¿ç”¨é»˜è®¤æ¨¡å‹ (qwen3.4b_base)
let mut text_gen = TextGeneration::default().await?;

let stream = text_gen.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±");
pin_mut!(stream);

while let Some(Ok(token)) = stream.next().await {
    print!("{}", token);
}
```

### è¿è¡Œæµ‹è¯•

```bash
# äº¤äº’å¼èŠå¤©
cargo test --lib pipe::tests::test_pipeline -- --nocapture

# é¢„è®¾å¯¹è¯
cargo test --lib pipe::tests::test_prompt -- --nocapture
```

### ç½‘ç»œé…ç½®

**ç¯å¢ƒå˜é‡æ–¹å¼ï¼ˆæ¨èï¼‰:**

```powershell
# Windows PowerShell - ä½¿ç”¨å›½å†…é•œåƒ
$env:HF_ENDPOINT = "https://hf-mirror.com"
```

**ä»£ç æ–¹å¼ï¼ˆå¯é€‰ï¼‰:**

```rust
use candle_llm_chat::utils::proxy::ProxyGuard;

let _proxy = ProxyGuard::new(7890); // è‡ªåŠ¨æ¸…ç†çš„ä»£ç†è®¾ç½®
```

## âš™ï¸ é…ç½®ä¸ä½¿ç”¨

### é€‰æ‹©æ¨¡å‹

```rust
// ä½¿ç”¨æ¶æ„é»˜è®¤æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3").await?;

// ä½¿ç”¨ GGUF é‡åŒ–æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3.8b_q4").await?;

// ä½¿ç”¨ Safetensors å®Œæ•´æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3.8b_base").await?;

// ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen3.4b_abliterated").await?;
```

### è‡ªå®šä¹‰æ¨ç†å‚æ•°

```rust
use candle_llm_chat::model::config::InferenceConfig;

let mut config = InferenceConfig::default();
config.temperature = 0.7;        // æ§åˆ¶éšæœºæ€§
config.sample_len = 2000;        // æœ€å¤§ç”Ÿæˆé•¿åº¦
config.repeat_penalty = 1.1;     // é‡å¤æƒ©ç½š

let mut text_gen = TextGeneration::new("qwen3", config).await?;
```

### é…ç½®æ–‡ä»¶

**`models.toml`** - æ¨¡å‹ä»“åº“é…ç½®ï¼š

```toml
# æ¶æ„çº§é…ç½®
[qwen3]

# åŸºç¡€æ¨¡å‹ (Safetensors)
[qwen3.4b_base]
model_repo = "Qwen/Qwen3-4B-Instruct-2507"
default = true  # æ¶æ„é»˜è®¤æ¨¡å‹

# é‡åŒ–æ¨¡å‹ (GGUF)
[qwen3.4b_q4]
model_repo = "byteshape/Qwen3-4B-Instruct-2507-GGUF"
model_file = "Qwen3-4B-Instruct-2507-Q4_K_S-3.66bpw.gguf"
# tokenizer_repo ä¼šè‡ªåŠ¨ä»å¯¹åº” base æ¨¡å‹è·å–

# è‡ªå®šä¹‰æ¨¡å‹
[qwen3.4b_abliterated]
model_repo = "huihui-ai/Huihui-Qwen3-4B-abliterated-v2"
tokenizer_repo = "huihui-ai/Huihui-Qwen3-4B-abliterated-v2"
```

### æ™ºèƒ½é…ç½®ç‰¹æ€§

- **è‡ªåŠ¨æ ¼å¼è¯†åˆ«**: ä»“åº“ååŒ…å« "GGUF" è‡ªåŠ¨è¯†åˆ«ä¸ºé‡åŒ–æ¨¡å‹
- **tokenizer_repo è‡ªåŠ¨å¡«å……**:
  - base æ¨¡å‹ï¼šè‡ªåŠ¨ä½¿ç”¨ model_repo
  - å…¶ä»–å˜ä½“ï¼šè‡ªåŠ¨ä»å¯¹åº” base æ¨¡å‹è·å–
- **çº¦å®šä¼˜äºé…ç½®**: éµå¾ª `æ¶æ„.å¤§å°_å˜ä½“` å‘½åè§„èŒƒ

> **æ³¨æ„**: é¡¹ç›®ç°åœ¨ä½¿ç”¨ç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ï¼Œä¸å†éœ€è¦ `config.toml` æ–‡ä»¶ã€‚HuggingFace Token ç­‰é…ç½®è¯·é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ã€‚æ¨¡å‹é…ç½®é€šè¿‡ `models.toml` ç®¡ç†ï¼Œæ”¯æŒæ™ºèƒ½çš„ tokenizer_repo è‡ªåŠ¨å¡«å……ã€‚

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·äº¤äº’å±‚"
        A[ç”¨æˆ·è¾“å…¥] --> B[TextGeneration::chat]
    end

    subgraph "é…ç½®ç®¡ç†å±‚"
        MR[ModelRegistry<br/>æ¨¡å‹æ³¨å†Œè¡¨] --> HI[HubInfo<br/>æ¨¡å‹ä»“åº“ä¿¡æ¯]
        MT[models.toml] --> MR
        HI --> ML[ModelLoader<br/>æ¨¡å‹åŠ è½½å™¨]
    end

    subgraph "æ ¸å¿ƒç»„ä»¶"
        C[ChatContext<br/>èŠå¤©ä¸Šä¸‹æ–‡ç®¡ç†] --> TG[TextGeneration<br/>æ–‡æœ¬ç”Ÿæˆç®¡é“]
        ML --> TG
        IC[InferenceConfig<br/>æ¨ç†é…ç½®] --> TG
        F[TokenOutputStream<br/>Tokenæµå¤„ç†] --> TG
        G[LogitsProcessor<br/>é‡‡æ ·å¤„ç†] --> TG
    end

    subgraph "æ¨¡å‹æŠ½è±¡å±‚"
        FW[ModelInference Trait<br/>ç»Ÿä¸€æ¨ç†æ¥å£] --> MW[ModelWeightså®ç°]
        MW --> MW1[quantized_qwen3::ModelWeights]
        MW --> MW2[qwen3::ModelForCausalLM]
    end

    subgraph "æ¨¡å‹å®ç°å±‚"
        MW1 --> H1[Qwen3 GGUFæ¨¡å‹æ–‡ä»¶]
        MW2 --> H2[Qwen3 Safetensorsæ¨¡å‹æ–‡ä»¶]
        I[Tokenizer<br/>åˆ†è¯å™¨] --> TG
    end

    subgraph "åº•å±‚æ¡†æ¶"
        K[Candle Framework<br/>æœºå™¨å­¦ä¹ æ¡†æ¶]
        L[CUDA Support<br/>GPUåŠ é€Ÿ]
        M[HuggingFace Hub<br/>æ¨¡å‹ä»“åº“]
    end

    subgraph "å·¥å…·ç»„ä»¶"
        N[ProxyGuard<br/>ä»£ç†è®¾ç½®] --> M
        O[gguf-utils<br/>æ¨¡å‹åˆ†ç‰‡åˆå¹¶] --> H1
        O --> H2
    end

    B --> C
    TG --> P[Stream Output<br/>æµå¼è¾“å‡º]
    P --> Q[å®æ—¶å“åº”æ˜¾ç¤º]

    H1 --> M
    H2 --> M
    I --> M
    MW1 --> K
    MW2 --> K
    K --> L

    style MR fill:#fff3e0
    style HI fill:#e3f2fd
    style FW fill:#f1f8e9
    style C fill:#e1f5fe
    style TG fill:#f3e5f5
    style P fill:#e8f5e8
```

### æ ¸å¿ƒè®¾è®¡

**é…ç½®é©±åŠ¨**: é€šè¿‡ `models.toml` ç®¡ç†æ¨¡å‹ï¼Œå­—ç¬¦ä¸²æ ‡è¯†ç¬¦é€‰æ‹© (`"qwen3"` æˆ– `"qwen3.8b_q4"`)

**ç»Ÿä¸€æ¥å£**: `ModelInference` trait æŠ½è±¡æ‰€æœ‰æ¨¡å‹æ¨ç†ï¼Œé€šè¿‡å®è‡ªåŠ¨å®ç°

**å¼‚æ­¥ä¼˜å…ˆ**: æ¨¡å‹åŠ è½½å’Œæ¨ç†å…¨å¼‚æ­¥ï¼ŒåŸºäº Tokio å’Œ async-stream

**æ™ºèƒ½é…ç½®**: tokenizer_repo è‡ªåŠ¨å¡«å……ï¼Œçº¦å®šä¼˜äºé…ç½®çš„è®¾è®¡ç†å¿µ

## æ‰©å±•æ–°æ¨¡å‹

æ·»åŠ æ–°æ¨¡å‹å˜ä½“åªéœ€åœ¨ `models.toml` ä¸­é…ç½®ï¼š

**GGUF é‡åŒ–æ¨¡å‹ï¼š**

```toml
[qwen3.32b_q4]
model_repo = "Qwen/Qwen3-32B-GGUF"
model_file = "Qwen3-32B-Q4_K_M.gguf"
# tokenizer_repo ä¼šè‡ªåŠ¨ä» qwen3.32b_base è·å–
```

**Safetensors å®Œæ•´æ¨¡å‹ï¼š**

```toml
[qwen3.32b_base]
model_repo = "Qwen/Qwen3-32B"
# model_file é»˜è®¤ä¸º "model.safetensors"
# tokenizer_repo è‡ªåŠ¨ä½¿ç”¨ model_repo
```

ç„¶åç›´æ¥ä½¿ç”¨ï¼š

```rust
let text_gen = TextGeneration::with_default_config("qwen3.32b_q4").await?;
let text_gen_full = TextGeneration::with_default_config("qwen3.32b_base").await?;
```

### æ·»åŠ æ–°æ¶æ„

1. åœ¨ `src/model/hub.rs` ä¸­æ·»åŠ æ–°çš„ `ModelArch` æšä¸¾å€¼
2. åœ¨ `src/model/config.rs` çš„ `ModelLoader` ä¸­æ·»åŠ åŠ è½½é€»è¾‘
3. åœ¨ `src/model/mod.rs` ä¸­ä¸ºæ–°æ¨¡å‹å®ç° `ModelInference` trait
4. åœ¨ `models.toml` ä¸­æ·»åŠ æ–°æ¶æ„çš„é…ç½®æ®µ

## ğŸ“Š å½“å‰å®ç°çŠ¶æ€

### âœ… å·²å®ç°

- **Qwen3 ç³»åˆ—å®Œæ•´æ”¯æŒ**: 4B/8B/14B/32B çš„ base å’Œ q4 å˜ä½“
- **æ™ºèƒ½é…ç½®ç®¡ç†**: tokenizer_repo è‡ªåŠ¨å¡«å……å’Œæ ¼å¼è¯†åˆ«
- **æµå¼èŠå¤© API**: åŸºäº async-stream çš„å®æ—¶è¾“å‡º
- **èŠå¤©ä¸Šä¸‹æ–‡ç®¡ç†**: MiniJinja æ¨¡æ¿æ”¯æŒ
- **æ¨ç†å‚æ•°é…ç½®**: æ¸©åº¦ã€é‡‡æ ·é•¿åº¦ã€é‡å¤æƒ©ç½šç­‰
- **ç½‘ç»œä»£ç†æ”¯æŒ**: ProxyGuard å’Œç¯å¢ƒå˜é‡é…ç½®

### ğŸš§ éƒ¨åˆ†å®ç°

- **Llama ç³»åˆ—**: é…ç½®å·²å‡†å¤‡ï¼Œä»£ç ä¸­æš‚æ—¶æ³¨é‡Š

### âŒ å¾…å®ç°

- **æ›´å¤šæ¨¡å‹æ¶æ„**: Llamaã€Mistral ç­‰
- **æ‰¹é‡æ¨ç†**: åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚
- **æ¨¡å‹é‡åŒ–å·¥å…·**: æœ¬åœ°é‡åŒ–æ”¯æŒ

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚
