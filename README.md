# candle-llm-chat

åŸºäº [Candle](https://github.com/huggingface/candle) æ¡†æ¶çš„ Rust LLM èŠå¤©åº“ï¼Œæ”¯æŒ GGUF é‡åŒ–æ¨¡å‹ã€æµå¼è¾“å‡ºå’Œ GPU åŠ é€Ÿã€‚

## âœ¨ ç‰¹æ€§

- ğŸ¯ **ç®€æ´ API**: å­—ç¬¦ä¸²æ ‡è¯†ç¬¦é€‰æ‹©æ¨¡å‹ `"qwen3"` / `"qwen3.W3_14b"`
- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ**: Qwen2/Qwen3/Llama ç³»åˆ—ï¼Œé€šè¿‡ `models.toml` é…ç½®
- ğŸ“¡ **æµå¼è¾“å‡º**: å®æ—¶æ‰“å­—æœºæ•ˆæœ
- ğŸš€ **GPU åŠ é€Ÿ**: CUDA æ”¯æŒ
- âš¡ **å¼‚æ­¥è®¾è®¡**: åŸºäº Tokio
- ğŸ§  **æ™ºèƒ½ä¸Šä¸‹æ–‡**: è‡ªåŠ¨è§’è‰²åˆ‡æ¢å’Œæ€è€ƒè¿‡ç¨‹è¿‡æ»¤

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Rust å·¥å…·é“¾ (æ¨èæœ€æ–°ç¨³å®šç‰ˆ)
- CUDA å·¥å…·åŒ… (å¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿ)
- `gguf-utils` (å¯é€‰ï¼Œç”¨äºåˆ†ç‰‡æ¨¡å‹åˆå¹¶): `cargo install gguf-utils`

### å®‰è£…

```bash
git clone https://github.com/your-username/candle-llm-chat.git
cd candle-llm-chat
```

### åŸºæœ¬ä½¿ç”¨

```rust
use candle_llm_chat::pipe::TextGeneration;
use futures_util::{StreamExt, pin_mut};

// ä½¿ç”¨é»˜è®¤æ¨¡å‹ (Qwen3-8B)
let mut text_gen = TextGeneration::default().await?;

let stream = text_gen.chat("ä½ å¥½");
pin_mut!(stream);

while let Some(Ok(token)) = stream.next().await {
    print!("{}", token);
}
```

### è¿è¡Œæµ‹è¯•

```bash
# äº¤äº’å¼èŠå¤©
cargo test --lib pipe::tests::test_pipeline -- --nocapture

# é¢„è®¾å¯¹è¯
cargo test --lib pipe::tests::test_prompt -- --nocapture
```

### ä»£ç†è®¾ç½® (å¯é€‰)

ä¸‹è½½ Hugging Face æ¨¡å‹æ—¶å¯èƒ½éœ€è¦ä»£ç†ï¼š

```rust
use candle_llm_chat::utils::proxy::ProxyGuard;

let _proxy = ProxyGuard::new(7890); // è‡ªåŠ¨æ¸…ç†çš„ä»£ç†è®¾ç½®
```

## âš™ï¸ é…ç½®ä¸ä½¿ç”¨

### é€‰æ‹©æ¨¡å‹

```rust
// ä½¿ç”¨æ¶æ„é»˜è®¤æ¨¡å‹
let text_gen = TextGeneration::with_default_config("qwen2").await?;

// ä½¿ç”¨ç‰¹å®šæ¨¡å‹å˜ä½“
let text_gen = TextGeneration::with_default_config("qwen3.W3_14b").await?;
let text_gen = TextGeneration::with_default_config("llama.DeepseekR1Llama8b").await?;
```

### è‡ªå®šä¹‰æ¨ç†å‚æ•°

```rust
use candle_llm_chat::model::config::InferenceConfig;

let mut config = InferenceConfig::default();
config.temperature = 0.7;        // æ§åˆ¶éšæœºæ€§
config.sample_len = 2000;        // æœ€å¤§ç”Ÿæˆé•¿åº¦
config.repeat_penalty = 1.1;     // é‡å¤æƒ©ç½š

let mut text_gen = TextGeneration::new("qwen3", config).await?;
```

### é…ç½®æ–‡ä»¶

**`models.toml`** - æ¨¡å‹ä»“åº“é…ç½®ï¼š

```toml
[qwen3.W3_8b]
model_repo = "Qwen/Qwen3-8B-GGUF"
model_file = "Qwen3-8B-Q4_K_M"
tokenizer_repo = "Qwen/Qwen3-8B"
default = true
```

**`config.toml`** - å…¨å±€é…ç½® (HuggingFace token ç­‰)

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·äº¤äº’å±‚"
        A[ç”¨æˆ·è¾“å…¥] --> B[TextGeneration::chat]
    end

    subgraph "é…ç½®ç®¡ç†å±‚"
        MR[ModelRegistry<br/>æ¨¡å‹æ³¨å†Œè¡¨] --> HI[HubInfo<br/>æ¨¡å‹ä»“åº“ä¿¡æ¯]
        MT[models.toml] --> MR
        HI --> ML[ModelLoader<br/>æ¨¡å‹åŠ è½½å™¨]
    end

    subgraph "æ ¸å¿ƒç»„ä»¶"
        C[ChatContext<br/>èŠå¤©ä¸Šä¸‹æ–‡ç®¡ç†] --> TG[TextGeneration<br/>æ–‡æœ¬ç”Ÿæˆç®¡é“]
        ML --> TG
        IC[InferenceConfig<br/>æ¨ç†é…ç½®] --> TG
        F[TokenOutputStream<br/>Tokenæµå¤„ç†] --> TG
        G[LogitsProcessor<br/>é‡‡æ ·å¤„ç†] --> TG
    end

    subgraph "æ¨¡å‹æŠ½è±¡å±‚"
        FW[Forward Trait<br/>ç»Ÿä¸€æ¨ç†æ¥å£] --> MW[ModelWeightså®ç°]
        MW --> MW1[quantized_qwen2::ModelWeights]
        MW --> MW2[quantized_qwen3::ModelWeights]
        MW --> MW3[quantized_llama::ModelWeights]
    end

    subgraph "æ¨¡å‹å®ç°å±‚"
        MW1 --> H1[Qwen2 GGUFæ¨¡å‹æ–‡ä»¶]
        MW2 --> H2[Qwen3 GGUFæ¨¡å‹æ–‡ä»¶]
        MW3 --> H3[Llama GGUFæ¨¡å‹æ–‡ä»¶]
        I[Tokenizer<br/>åˆ†è¯å™¨] --> TG
    end

    subgraph "åº•å±‚æ¡†æ¶"
        K[Candle Framework<br/>æœºå™¨å­¦ä¹ æ¡†æ¶]
        L[CUDA Support<br/>GPUåŠ é€Ÿ]
        M[HuggingFace Hub<br/>æ¨¡å‹ä»“åº“]
    end

    subgraph "å·¥å…·ç»„ä»¶"
        N[ProxyGuard<br/>ä»£ç†è®¾ç½®] --> M
        O[gguf-utils<br/>æ¨¡å‹åˆ†ç‰‡åˆå¹¶] --> H1
        O --> H2
        O --> H3
    end

    B --> C
    TG --> P[Stream Output<br/>æµå¼è¾“å‡º]
    P --> Q[å®æ—¶å“åº”æ˜¾ç¤º]

    H1 --> M
    H2 --> M
    H3 --> M
    I --> M
    MW1 --> K
    MW2 --> K
    MW3 --> K
    K --> L

    style MR fill:#fff3e0
    style HI fill:#e3f2fd
    style FW fill:#f1f8e9
    style C fill:#e1f5fe
    style TG fill:#f3e5f5
    style P fill:#e8f5e8
```

### æ ¸å¿ƒè®¾è®¡

**é…ç½®é©±åŠ¨**: é€šè¿‡ `models.toml` ç®¡ç†æ¨¡å‹ï¼Œå­—ç¬¦ä¸²æ ‡è¯†ç¬¦é€‰æ‹© (`"qwen3"` æˆ– `"qwen3.W3_14b"`)

**ç»Ÿä¸€æ¥å£**: `Forward` trait æŠ½è±¡æ‰€æœ‰æ¨¡å‹æ¨ç†ï¼Œé€šè¿‡å®è‡ªåŠ¨å®ç°

**å¼‚æ­¥ä¼˜å…ˆ**: æ¨¡å‹åŠ è½½å’Œæ¨ç†å…¨å¼‚æ­¥ï¼ŒåŸºäº Tokio å’Œ async-stream

## æ‰©å±•æ–°æ¨¡å‹

æ·»åŠ æ–°æ¨¡å‹å˜ä½“åªéœ€åœ¨ `models.toml` ä¸­é…ç½®ï¼š

```toml
[qwen3.W3_72b]
model_repo = "Qwen/Qwen3-72B-GGUF"
model_file = "Qwen3-72B-Q4_K_M"
tokenizer_repo = "Qwen/Qwen3-72B"
```

ç„¶åç›´æ¥ä½¿ç”¨ï¼š

```rust
let text_gen = TextGeneration::with_default_config("qwen3.W3_72b").await?;
```

æ·»åŠ æ–°æ¶æ„éœ€è¦åœ¨ `ModelLoader::load()` ä¸­å®ç°åŠ è½½é€»è¾‘ã€‚

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚é˜… [LICENSE](LICENSE) æ–‡ä»¶ã€‚
